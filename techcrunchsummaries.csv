Title,Text
OmniAI transforms business data for AI,"OmniAI, founded by Anna Pojawis and Tyler Maran, aims to help companies extract value from their data by transforming unstructured enterprise data into formats that AI and data analytics tools can understand. Many companies struggle with data analytics due to technical and security roadblocks, especially in regulated industries like healthcare and finance. OmniAI syncs with data storage services and databases, allowing companies to run the model of their choice on the transformed data. The company recently closed a $3.2 million seed round and already has 10 customers, with annual recurring revenue expected to reach $1 million by 2025. OmniAI offers integrations with various models for tasks like redacting sensitive information and building AI-powered applications, with a focus on running models alongside existing infrastructure."
Mayfield allocates $100M to AI incubator modeled after its entrepreneur-in-residence program,"Mayfield Fund is launching AI Garage, a $100 million initiative for founders interested in building ""AI teammate"" companies, modeled after its entrepreneur-in-residence program. The firm plans to welcome up to five aspiring founders every six months, providing them with support to turn their raw concepts into fundable companies. Participants won't receive capital on day one, but will be allocated between $1 million and $5 million once the business plan is developed. Mayfield's interest in AI application startups, specifically in the area of ""AI teammates,"" is driving the expansion and formalization of its EIR program. The firm believes AI teammates can collaborate with humans on complex tasks to elevate performance, asserting that labeling them as teammates is a clever marketing tactic for building human-friendly AI. Mayfield has already invested in several AI teammate companies, such as DevRev and Docket, with the goal of shaping the future of the workplace through collaborative intelligence between AI and humans."
"At the AI Film Festival, humanity triumphed over tech","At the AI Film Festival, AI was used in various forms in the top 10 finalists' movies, such as AI-generated backdrops, animations, voice-overs, and special effects. The limitations of current AI tools were evident, with some films being hampered by underwhelming effects. The human touch, such as skilled direction and emotional performances, often made a significant difference in the effectiveness of the films. Despite some shortcomings, films like ""Where Do Grandmas Go When They Get Lost?"" stood out due to heartfelt scripts and strong performances. Ultimately, the festival highlighted that while AI can assist in filmmaking, the human element, including emotionality and creativity, remains irreplaceable."
Microsoft dodges UK antitrust scrutiny over its Mistral AI stake,"Microsoft's recent investment in Mistral AI avoids antitrust scrutiny in the UK, with the Competition and Markets Authority concluding that it does not qualify for investigation. The CMA has been conducting probes into AI investments and partnerships by tech giants like Microsoft and Amazon. Microsoft's investment in Mistral AI was seen as a move to rival OpenAI. The CMA has expressed concerns about Big Tech's tactics to avoid regulatory oversight through partnerships. While the Mistral AI case did not qualify for investigation, the CMA is still looking into other AI partnerships involving Microsoft and Amazon."
EU calls for help with shaping rules for general-purpose AIs,"EU is seeking input from stakeholders on rules for general-purpose AI models under the AI Act, with a deadline of April 2025 for compliance. The consultation covers transparency, risk assessment, and monitoring of GPAIs. GPAI providers can shape the template for model training content summaries. Expressions of interest are also sought for drafting the Code, with workshops and meetings planned. Concerns about civil society exclusion in the drafting process are being addressed by the Commission."
Study suggests that even the best AI models hallucinate a bunch,"Generative AI models like GPT-4o often hallucinate, with even the best models generating inaccurate information about 65% of the time. A recent study by researchers from Cornell, UW, and AI2 found that models struggle with factual accuracy, especially when answering questions not found on Wikipedia. Despite claims from OpenAI and other AI players, models continue to struggle with hallucinations, showing similar performance across different models. The size of the model did not impact its ability to hallucinate, with smaller models performing comparable to larger models. To reduce hallucinations, researchers suggest incorporating human-in-the-loop fact-checking and developing advanced fact-checking tools for generative AI models. Improvements in the accuracy of AI models are necessary, as current benchmarks are not sufficient in evaluating their performance."
Here are India’s biggest AI startups based on how much money they’ve raised,"India's AI startup ecosystem is rapidly growing with a focus on solving local problems and integrating local languages. Funding for Indian AI startups dropped by nearly 80% in 2023 to $113.4 million, while funding in the U.S. grew to $16.2 billion. Key Indian AI startups include Krutrim, Sarvam AI, Mad Street Den, Wysa, and Neysa Networks. Global and local investors are actively scouting for AI startups in India, with Lightspeed India and SEA alone investing over $150 million in AI in the last 18 months. Indian AI startups are also expanding their reach beyond India, with some looking to enter the U.S. market."
Women in AI: Allison Cohen on building responsible AI projects,"Allison Cohen, a senior applied AI projects manager at Mila, is highlighted for her work in deploying socially beneficial AI projects, including a tool to detect misogyny and recommend sustainable farming practices. She emphasizes the importance of interdisciplinary collaboration in building responsible AI applications. Cohen navigates the challenges of the male-dominated tech industry by finding allies and creating platforms, like the podcast ""The World We're Building,"" to elevate the work of women and non-binary individuals in AI. She advises women looking to enter the AI field to find opportunities to build their voice and stand out, even through volunteering. Some pressing issues facing AI as it evolves include labor exploitation in data labeling and the need for responsible AI built with values that align with the interests of local communities. To promote responsible AI, investors are encouraged to ask about a team's values and accountability to the community to ensure ethical practices."
Exactly.ai secures $4M to help artists use AI to scale up their output,"London-based startup Exactly.ai has raised $4.3 million in seed funding to help artists use generative AI to scale up their output. The company, founded in 2022 by Tonia Samsonova, allows artists to retain legal ownership of their art and reproduce designs faster. Exactly.ai currently has 40,000 registered users and aims to cater to the growing global generative AI market. Artists pay a subscription fee to use the platform and can quadruple their income by serving more demand. The startup's unique algorithm is based on a combination of Picsart's model and training data provided by artists and museums."
"Ilya Sutskever, OpenAI co-founder and longtime chief scientist, departs","Ilya Sutskever, co-founder and chief scientist at OpenAI, has left the company with a replacement named Jakub Pachocki, who joined in 2017 and has been promoted several times. The Superalignment team, focused on regulating superintelligent AI systems, will see changes in leadership following Sutskever's departure. Sutskever's exit comes after disagreements over the company's direction and concerns about CEO Sam Altman's behavior, leading to a major shakeup within OpenAI's leadership last November. Sutskever, a highly accomplished figure in AI, left Google Brain to join OpenAI in 2015 and is now pursuing a project of personal significance. Altman was eventually reinstated after the tumultuous events, and Sutskever has not returned to work since then."
SewerAI uses AI to spot defects in sewer pipes,"Climate change is leading to more sewage failures due to floods overwhelming wastewater systems and America's outdated infrastructure. SewerAI, founded by Matthew Rosenthal and Billy Gilmartin, uses AI to automate sewer inspection data capture and defect tagging. The company sells cloud-based, AI-powered products for field inspections and data management of sewer infrastructure. SewerAI stands out in the market due to the quality of its model training data, which includes footage of inspections of 135 million feet of pipes. The company recently raised $15 million in funding, which will be used for go-to-market expansion, AI model training, hiring, and expanding its product portfolio."
"Generative AI is coming for healthcare, and not everyone’s thrilled","Generative AI is increasingly being integrated into healthcare by Big Tech firms and startups like Google, Amazon, and Microsoft. Startups in healthcare, like Ambience Healthcare and Abridge, have raised millions in venture capital for generative AI efforts. However, professionals and patients are split on whether healthcare-focused generative AI is ready for widespread use. Concerns exist around generative AI's limitations in handling complex medical queries and emergencies, with studies showing high error rates in diagnosing diseases. While generative AI has shown promise in specific areas like medical imaging, technical and compliance roadblocks must be addressed before it can be trusted as an assistive healthcare tool. The World Health Organization advocates for rigorous science, human oversight, and safeguards to prevent potential harm to patients and the healthcare industry from widespread implementation of generative AI in healthcare."
Securing generative AI across the technology stack,"By 2026, over 80% of enterprises will be using generative AI models, APIs, or applications. Only 38% of companies using generative AI today mitigate cybersecurity risks. The user interface of AI applications is susceptible to prompt injections. Security leaders are under pressure to enable GenAI applications within organizations. Data security tools often rely on preset rules, leading to false positives. The application layer of enterprise AI infrastructure is less mature, leading ML teams to rely on existing tools like Amazon SageMaker. Regulation is expected to play a role in shaping the landscape of AI security. Generative AI applications are vulnerable to threats like data poisoning and leakage. Data quality and privacy will raise significant challenges in an AI-first world. AI security platforms will be crucial as organizations increasingly rely on generative AI capabilities."
"Women in AI: Sarah Myers West says we should ask, ‘Why build AI at all?’","TechCrunch is featuring interviews with women in AI to highlight their contributions. Sarah Myers West, of the AI Now Institute, emphasizes the need to question why AI is being built in the first place. She focuses on the social implications of AI and the concentration of power in the tech industry. West's work examines how AI is used in various contexts and the need for greater testing and validation to avoid harmful errors. Responsible AI development involves questioning the necessity of AI for a specific purpose and ensuring compliance with the law. Ultimately, the end use of AI technology should be a primary consideration in building AI."
Cohere raises $500M to beat back generative AI rivals,"Cohere, a generative AI startup, has raised $500 million in new funding from investors like Cisco and AMD, valuing the company at $5.5 billion. The company has raised a total of $970 million, with plans for accelerated growth and expanding technical teams. Cohere focuses on delivering real-world benefits for businesses through AI customization for tasks like summarizing documents and powering chatbots. Its cloud-agnostic platform can be deployed in various cloud environments or onsite, and it works closely with customers to create tailored AI models based on their data. With a growing customer base and plans to double its employee headcount, Cohere is positioned as a strong contender in the generative AI space."
WTF is AI?,"Artificial intelligence (AI) is software that approximates human thinking, using large statistical models to predict patterns. AI models like ChatGPT map out language patterns but don't actually understand language. AI models can quickly generate low-value written work and assist with coding tasks. However, challenges like bias and stolen training data pose ethical issues. While AI advancements are impressive, achieving artificial general intelligence (AGI) remains a distant concept. The future of AI innovation and its potential impact on society are uncertain."
Orby is building AI agents for the enterprise,"Orby AI is developing AI agents that can autonomously perform tasks such as data entry and document processing, aiming to revolutionize business workflows. The founders, Bella Liu and William Lu, bring experience from IBM, UiPath, Nvidia, and Google Cloud to create a platform that learns and acts on workflows in real time. Orby's cloud-based generative AI model utilizes symbolic AI to adapt to changes in workflows intelligently. Despite competition in the sector, Orby raised $30 million in a Series A funding round and plans to expand its team and go-to-market strategy. The company prioritizes customer data privacy, using telemetry data for model improvement while keeping humans in the feedback loop."
UK opens office in San Francisco to tackle AI risk,"The UK is expanding efforts in AI safety by opening a second location in San Francisco to be closer to the epicenter of AI development. The AI Safety Institute, launched in November 2023, currently has 32 employees and recently released tools for testing the safety of foundational AI models. Despite the challenge of benchmarking models, the institute aims to engage with AI companies to evaluate them and make AI safe across society. The UK plans to develop more AI legislation in the future, but will only do so once the scope of AI risks is better understood. The institute's goal is to take an international approach to AI safety, collaborate with other countries, and incentivize more research globally."
The top AI features Apple announced at WWDC 2024,"Apple announced several AI features at WWDC 2024, including a revamped Siri that can now handle speech stumbles and better understand context. The new Siri can also type responses and take actions in and across apps. ChatGPT, a chatbot experience from OpenAI, will be integrated into Siri and other first-party apps on Apple devices. Genmoji, a feature coming to iOS 18, allows users to create AI emoji-like images of people in their photo library. The upgraded Photos app includes tools like Clean Up to remove unwanted elements and better organization of photos using AI. iOS 18 will introduce optional call transcription and summaries in the Notes app for iPhone 15 Pro and newer models."
EasyTranslate thinks augmenting LLMs with humans will give it an edge over pure AI translation services,"EasyTranslate has shifted towards an AI-driven platform called “HumanAI” to improve content translation efficiency by combining human expertise with large language models (LLMs). This platform utilizes LLMs to generate short-term memory for more accurate translations, only involving humans where needed. By leveraging LLMs and customer data, EasyTranslate can offer customized content translations at a fraction of the cost, pricing at €0.01 per word. The company aims to provide added value by combining AI with human feedback, standing out against pure AI-based solutions in the market. Backed by €3 million in funding, EasyTranslate seeks to compete in the global translation market by offering efficient and cost-effective services to businesses like Wix and Monday.com."
Apple signs the White House’s commitment to AI safety,"Apple has signed the White House's commitment to AI safety, joining 15 other tech companies in developing safe, secure, and trustworthy AI. The company will soon launch Apple Intelligence, a generative AI offering, to its 2 billion users. By embedding ChatGPT in the iPhone, Apple is going all in on generative AI. The commitment includes stress testing AI models before public release, treating model weights confidentially, and developing content labeling systems. The White House's stance on open-source AI models could impact the industry significantly, as federal agencies have made progress in AI-related tasks following President Biden's executive order."
Level AI applies algorithms to contact center pain points,"Ashish Nagar, a former Amazon Alexa engineer, founded Level AI to apply AI algorithms to contact center tasks and improve productivity. The platform automates customer service tasks and provides insights for managers and agents, such as scoring agents on metrics like total conversations and ""dead air."" Level AI can show hints to agents during conversations, gauge customer sentiment, and offer coaching tools to improve performance. Despite concerns about data privacy and job displacement, Level AI offers flexibility for customers to control and manage their data. The company has attracted customers like Affirm and Penske, and recently closed a $39.4 million Series C funding round to expand its platform to new customer segments. With a growing market for contact center software expected to reach $145.20 billion by 2029, Level AI aims to continue innovating and expanding its workforce to meet demand."
"Ilya Sutskever, OpenAI’s former chief scientist, launches new AI company","Ilya Sutskever, former chief scientist at OpenAI, has launched a new AI company, Safe Superintelligence Inc. (SSI), with former Y Combinator partner Daniel Gross and ex-OpenAI engineer Daniel Levy. Sutskever left OpenAI after a falling out with leadership over AI safety approaches, along with co-worker Jan Leike, who now heads a team at rival AI shop Anthropic. Sutskever has long emphasized the importance of AI safety, predicting that superintelligent AI may arrive within the decade and emphasizing the need for research into controlling and restricting it. SSI, unlike OpenAI, is for-profit and designed to advance capabilities rapidly while prioritizing safety, with a focus on revolutionary engineering and scientific breakthroughs. The company is currently recruiting technical talent in Palo Alto and Tel Aviv and is likely to attract significant funding due to interest in AI and the team's credentials."
Grammy CEO says music industry also has AI concerns,"Harvey Mason Jr., CEO of the Recording Academy, announced that the Grammys would accept music made with AI in the creative process, leading to concerns about the music industry's future. AI is already being used in music for mastering and equalizing sounds, raising issues about copyrights, royalties, and fair compensation for artists. Mason launched the Human Artistry Campaign to address these concerns and advocate for more regulations around the use of AI in music. The industry is grappling with how to protect artists from unauthorized AI use, with recent incidents involving Drake and Tupac's estate sparking legal battles. While some embrace AI in music, others, like Devante, the Artist, believe it shouldn't exist and call for more safeguards to protect human creativity."
Women in AI: Miriam Vogel stresses the need for responsible AI,"Miriam Vogel, CEO of EqualAI, emphasizes the importance of responsible AI governance and reducing unconscious bias in AI. Vogel's background in law and policy brings a unique perspective to the AI industry. She advocates for more diversity and inclusion in AI development to ensure AI works for all consumers. AI literacy is essential for everyone to thrive in an AI-powered economy. Standardized measures and metrics are needed to evaluate AI systems and build trust. Users should be aware of bias and potential harms in AI, and building AI responsibly requires asking key questions and following best practices. Investors play a crucial role in pushing for responsible AI to ensure safety, effectiveness, and public trust."
Why code-testing startup Nova AI uses open source LLMs more than OpenAI,"Nova AI, a startup focused on code-testing, is using open source models like Llama and StarCoder instead of OpenAI's GPT-4. The company aims to target mid-size to large enterprises with complex code bases. They use their own open source embedding models to avoid sending customer data to OpenAI and rely on open source AI models for writing tests. By avoiding OpenAI, Nova AI not only appeases enterprise concerns about data privacy but also finds open source models to be cheaper and more effective for specific tasks. The company's co-founders have backgrounds in big tech companies like Google and Meta, and are confident in the rapid progress of open source AI models."
"Exa raises $17M from Lightspeed, Nvidia, Y Combinator to build a Google for AIs","Exa is building a unique search engine that caters to AI models rather than humans, allowing them to search for information and return accurate answers. The startup recently raised $17 million in Series A funding led by Lightspeed, Nvidia, and Y Combinator, bringing their total funding to $22 million. The founders, Harvard alumni Will Bryk and Jeff Wang, initially focused on improving search using AI before pivoting to serve AI companies who needed an API version of their search engine. Their product has gained significant traction, serving thousands of developers and catering to various use cases including training data curation for companies like Databricks. While not aiming to upend Google, Exa's approach to AI-focused search engines could pose a threat to traditional search hegemony in the future."
"‘Emotion AI’ may be the next trend for business software, and that could be problematic","Businesses are increasingly turning to ""emotion AI"" to help AI bots better understand human emotion, as predicted by PitchBook's Enterprise SaaS Emerging Tech Research report. Emotion AI, a more advanced form of sentiment analysis, uses sensors, machine learning, and psychology to detect human emotion during interactions. Major AI cloud providers offer emotion AI capabilities, making it more accessible in the business world. Startups like Uniphore and MorphCast are emerging to meet the demand for emotion AI technology. However, there are concerns about the effectiveness of emotion AI, as research suggests that human emotion cannot be accurately determined by facial movements. Additionally, AI regulations like the European Union's AI Act may limit the use of emotion detection systems. Silicon Valley is pushing for AI bots with emotional understanding, but the effectiveness of this technology remains uncertain."
"SUSE wants a piece of the AI cake, too","SUSE, a well-known open source company in Europe, is aiming to break into the U.S. market with its AI solutions and strategy. The company's CEO highlighted the importance of open source in driving innovation and evolution in the tech industry. SUSE has seen success in attracting former CentOS users after Red Hat's changes, showing the appeal of their offerings. The company's AI solution focuses on enabling businesses to put AI workloads into production securely and privately, catering to the need for data control and compliance. Modular and open-source, the SUSE AI solution has already attracted customers like Fujitsu."
MIT researchers release a repository of AI risks,"MIT researchers have developed an AI risk repository with over 700 AI risks grouped by causal factors, domains, and subdomains to help policymakers, stakeholders, and the AI industry. Existing risk frameworks cover only a fraction of the risks identified in the repository, highlighting the need for a more comprehensive approach to understanding AI risks. The repository aims to save time, increase oversight, and provide a foundation for more specific work in AI risk evaluation. The researchers plan to evaluate how different AI risks are being addressed in the next phase of their research using the repository. While alignment on AI risks is important, it may not be sufficient to spur competent regulation of AI systems."
